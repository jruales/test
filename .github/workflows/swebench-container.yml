name: swebench-eval

on:
   workflow_dispatch:
    inputs:
      run_id:
        required: true
        default: 'validate-gold'
      predictions_path:
        required: true
        default: 'gold'
      imageIds:
        required: true
        default: 'sympy__sympy-20590'
      max_workers:
        required: true
        default: 1  
      repoName:
        description: Name of the repo
        required: false
      patch:
        description: 'Base64 encoded patch content'
        required: false
        default: ''
      command:
        description: 'Base64 encoded command content'
        required: false
        default: ''
      ref:
        description: 'The branch, tag, or commit SHA to checkout'
        required: false
        default: '9802a2c638ac5cb2be7120651835ffb4f82a8d60' # snapshot from 7/27/24

permissions:
      id-token: write
      contents: read      
env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  run_swebench:    
    env:
      REPO_NAME: princeton-nlp/SWE-bench
      REPO_PATH: SWE-bench-repo
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest]
        python-version: ["3.12"]
    runs-on: ${{ matrix.os }}
    defaults:
      run:
        working-directory: ${{ env.REPO_PATH }}    
    steps:  
    - name: Checkout repository ${{ env.REPO_NAME }}
      uses: actions/checkout@v4
      with:
        repository: ${{ env.REPO_NAME }}
        path: ${{ env.REPO_PATH }}
    - name: Setup Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    - name: Install SWE-bench
      run: |
        pip install --upgrade pip wheel
        pip install -e .
    - name: 'Az CLI login'
      if: ${{ github.event.inputs.command != '' }}
      uses: azure/login@v2
      with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}  
    - name: Login to ACR
      if: ${{ github.event.inputs.command != '' }}
      run: az acr login --name codeexecservice
    - name: Prepare docker container
      if: false
      shell: bash
      run: |
            python3 << 'EOF'
            import re
            import docker
            import os
            import sys            
            import resource
            import traceback
            import logging
            
            from argparse import ArgumentParser
            from pathlib import Path
            from swebench.harness.constants import (KEY_INSTANCE_ID)
            from swebench.harness.test_spec import make_test_spec
            from swebench.harness.utils import load_swebench_dataset, str2bool
            from swebench.harness.docker_build import (build_container, setup_logger, build_env_images)
            from swebench.harness.prepare_images import main as prepare_images
            from swebench.harness.docker_utils import (list_images, copy_to_container)
            from swebench.harness.run_evaluation import (get_gold_predictions, get_dataset_from_preds)

            def main(
                    dataset_name: str,
                    split: str,
                    instanceId: str,
                    predictions_path: str,
                    max_workers: int,
                    force_rebuild: bool,
                    cache_level: str,
                    clean: bool,
                    open_file_limit: int,
                    run_id: str,
                    timeout: int,
                ):
                """
                Run evaluation harness for the given dataset and predictions.
                """
                # set open file limit
                assert len(run_id) > 0, "Run ID must be provided"
                resource.setrlimit(resource.RLIMIT_NOFILE, (open_file_limit, open_file_limit))
                client = docker.from_env()
            
                # load predictions as map of instance_id to prediction
                predictions = get_gold_predictions(dataset_name, split)                
                predictions = {pred[KEY_INSTANCE_ID]: pred for pred in predictions}
            
                # get dataset from predictions
                instance_ids=[instanceId]
                dataset = get_dataset_from_preds(dataset_name, split, instance_ids, predictions, run_id)
                # full_dataset = load_swebench_dataset(dataset_name, split)
                existing_images = list_images(client)
                print(f"Running {len(dataset)} unevaluated instances...")
                if not dataset:
                    print("No instances to run.")
                else:
                    # build environment images + run instances
                    build_env_images(client, dataset, force_rebuild, max_workers)
                    # run_instances(predictions, dataset, cache_level, clean, force_rebuild, max_workers, run_id, timeout)
                    # Build + start instance container (instance image should already be built)
                    log_file = Path("./run_instance.log")
                    logger = setup_logger(instanceId, log_file)
                    test_spec = make_test_spec(dataset[0])
                    container = build_container(test_spec, client, run_id, logger, nocache=True, force_rebuild=False)
                    container.start()
                    logger.info(f"Container for {instanceId} started: {container.id}")
                    eval_file = Path("./eval.sh")
                    eval_file.write_text(test_spec.eval_script)
                    env_file = Path("./setupenv.sh")
                    env_file.write_text(test_spec.setup_env_script)
                    repo_file = Path("./setupRepo.sh")
                    repo_file.write_text(test_spec.install_repo_script)
                    print(f"========================")
                    print(f"envScript={test_spec.setup_env_script}")
                    print(f"repoScript={test_spec.install_repo_script}")
                    print(f"env_image_key={test_spec.env_image_key}, base_image_key={test_spec.base_image_key}, instance_image_key={test_spec.instance_image_key}")
                    print(f"env_dockerfile={test_spec.env_dockerfile}")
                    print(f"instance_dockerfile={test_spec.instance_dockerfile}")
                    print(f"========================")
                    print(f"Eval script for {instanceId} written, now applying to container...")
                    copy_to_container(container, eval_file, Path("testbed/eval.sh"))
            
            if __name__ == "__main__":
                imageId="${{ github.event.inputs.imageIds }}"
                parser = ArgumentParser()
                parser.add_argument("--dataset_name", default="princeton-nlp/SWE-bench_Lite", type=str, help="Name of dataset or path to JSON file.")
                parser.add_argument("--split", type=str, default="test", help="Split of the dataset")
                parser.add_argument("--instanceId", type=str, default=imageId, help="Instance IDs to run (space separated)")
                parser.add_argument("--predictions_path", type=str, default="gold", help="Path to predictions file - if 'gold', uses gold predictions")
                parser.add_argument("--max_workers", type=int, default=4, help="Maximum number of workers (should be <= 75%% of CPU cores)")
                parser.add_argument("--open_file_limit", type=int, default=4096, help="Open file limit")
                parser.add_argument(
                    "--timeout", type=int, default=1_800, help="Timeout (in seconds) for running tests for each instance"
                    )
                parser.add_argument(
                    "--force_rebuild", type=str2bool, default=False, help="Force rebuild of all images"
                )
                parser.add_argument(
                    "--cache_level",
                    type=str,
                    choices=["none", "base", "env", "instance"],
                    help="Cache level - remove images above this level",
                    default="env",
                )
                # if clean is true then we remove all images that are above the cache level
                # if clean is false, we only remove images above the cache level if they don't already exist
                parser.add_argument(
                    "--clean", type=str2bool, default=False, help="Clean images above cache level"
                )
                parser.add_argument("--run_id", type=str, default="gold", help="Run ID - identifies the run")
                args = parser.parse_args()
            
                main(**vars(args))
            EOF
    - name: Prepare docker container -old
      if: false
      shell: bash
      run: |
            python3 << 'EOF'
            import re
            import docker
            import os
            import sys
            
            from argparse import ArgumentParser
            from pathlib import Path
            from swebench.harness.test_spec import make_test_spec
            from swebench.harness.utils import  load_swebench_dataset
            from swebench.harness.docker_build import (build_container, setup_logger)
            from swebench.harness.prepare_images import main as prepare_images
            
            def matches_filter(instance_id, instance_filter):
                if instance_filter is None:
                    return True
                return re.fullmatch(instance_filter, instance_id) is not None
            
            def main(
                dataset_name,
                instance_filter,
                instance_ids_file,
                run_id):
            
                print("Reading SWE data...")
                dataset = load_swebench_dataset(dataset_name, "test")
            
                print("Setting up Instances...")
                log_file = Path("./run_instance.log")
                client = docker.from_env()
                with open(instance_ids_file, mode="w", encoding="utf-8") as file:
                    for instance in dataset:
                        instance_id = instance['instance_id']
                        if matches_filter(instance_id, instance_filter):
                            file.write(f"{instance_id}\n")
            
                            # Prepare instance image throws an exception if the image already exists.
                            try:
                                prepare_images(dataset_name, split="test", instance_ids=[instance_id], max_workers=4, force_rebuild=False, open_file_limit=8192)
                            except Exception:
                                pass
            
                            # Build the container with the instance
                            swe_logger = setup_logger(instance_id, log_file)
                            test_spec = make_test_spec(instance)
                            container = build_container(test_spec, client, run_id=run_id, logger=swe_logger, nocache=True, force_rebuild=False)
                            container_id = container.id[:12]
                            file.write(f"{container_id}\n")
                            container.start()
                            print(f"Container created for {instance_id}:\n  Container Id: {container_id}\n  Container Name: {container.name}\n  Image Id: {container.image.id}")
            
            if __name__ == "__main__":
                instanceId="${{ github.event.inputs.imageIds }}"
                print(f"instanceId from input: {instanceId}") 
                parser = ArgumentParser()
                parser.add_argument("--dataset_name", type=str, default="swe-bench-lite", help="Name of the dataset to use")
                parser.add_argument("--instance_filter", type=str,default=instanceId, help="Regular expression instance ID filter to run")
                parser.add_argument("--instance_ids_file", type=str, default="instance_ids.txt", help="File to write instance ids")
                parser.add_argument("--run_id", type=str, default="gold", help="Run ID - identifies the run")
                args = parser.parse_args()
                main(**vars(args))
            EOF

    - name: Run SWEBench docker
      if: ${{ github.event.inputs.command != '' }}
      shell: bash
      run: |
            imageId="${{ github.event.inputs.imageIds }}"
            echo "imageid=${imageId}" 
            imageName="sweb.eval.x86_64.${imageId}"
            echo "imageName=${imageName}"
            docker run --name $imageId -d codeexecservice.azurecr.io/${imageName}:latest tail -f /dev/null
            docker ps
    - name: Apply patch to SWE Bench Container
      if: ${{ github.event.inputs.command != '' }}
      shell: bash
      run: |
          if [ -n "${{ github.event.inputs.patch }}" ]; then
            echo "Patch input provided. Applying patch..."
            echo "${{ github.event.inputs.patch }}" | base64 --decode | sed 's/\r$//'  > patch.diff    
            echo "Decoded patch content:"
            cat patch.diff
            docker images
            docker ps
            echo "copying patch file..."
            imageId="${{ github.event.inputs.imageIds }}"
            containerName="${imageId}"
            # containerName=sweb.eval.$imageId.gold
            echo "containerName=${containerName}"
            working_dir=$(docker exec $containerName pwd)
            echo "workingDir:$working_dir"
            GIT_STATUS_OUTPUT=$(docker exec $containerName bash -c "git log -1 2>&1")
            echo "git status: $GIT_STATUS_OUTPUT"
            CMD_OUTPUT0=$(docker exec $containerName bash -c "source /opt/miniconda3/bin/activate && conda activate testbed && git log -1")
            echo $CMD_OUTPUT0      
            docker cp patch.diff $containerName:${working_dir}/
            echo "Apply the patch:"
            PATCH_OUTPUT=$(docker exec $containerName bash -c "git apply --verbose patch.diff 2>&1")
            echo "patch Output: $PATCH_OUTPUT"
            LS_OUTPUT=$(docker exec $containerName ls)
            echo "ls Output: $LS_OUTPUT"
          else
            echo "No patch input provided. Skipping patch application."
          fi         
        
    - name: Run custom command on SWE Bench Container
      shell: bash
      run: |       
         if [ -n "${{ github.event.inputs.command }}" ]; then
           echo "Start running custom command"         
           echo "${{ github.event.inputs.command }}"
           cmd=$(echo "${{ github.event.inputs.command }}" | base64 --decode | sed 's/\r$//')
           echo "Decoded custom command is:"   
           echo $cmd
           imageId="${{ github.event.inputs.imageIds }}"
           # containerName=sweb.eval.$imageId.gold
           containerName="${imageId}"
           echo "Command output:"   
           CMD_OUTPUT0=$(docker exec $containerName bash -c "source /opt/miniconda3/bin/activate && conda activate testbed && eval $cmd")
           echo $CMD_OUTPUT0
           echo "RAN_CUSTOM_COMMAND=true" >> $GITHUB_ENV
           echo "Finished running command!"
         fi
    - name: Run swebench harness evaluation (ces)
      if: env.RAN_CUSTOM_COMMAND != 'true'      
      run: |
        python -m swebench.harness.run_evaluation \
          --predictions_path ${{ github.event.inputs.predictions_path }} \
          --max_workers ${{ github.event.inputs.max_workers }} \
          --instance_ids ${{ github.event.inputs.imageIds }} \
          --run_id ${{ github.event.inputs.run_id }}
    - name: Get SWEBench eval test result (ces)
      if: env.RAN_CUSTOM_COMMAND != 'true'
      run: |        
        ls -R ./logs/run_evaluation/${{ github.event.inputs.run_id }}/${{ github.event.inputs.predictions_path }}
        logsDir="./logs/run_evaluation/${{ github.event.inputs.run_id }}/${{ github.event.inputs.predictions_path }}/${{ github.event.inputs.imageIds }}"
        echo "Contents of test log:"
        # cat "$logsDir/run_instance.log"
        # echo "=="
        # cat "$logsDir/report.json"
        echo "========"
        cat "$logsDir/test_output.txt"
        echo "===DONE====="
