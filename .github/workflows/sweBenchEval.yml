name: swebench-eval

on:
   workflow_dispatch:
    inputs:
      choose_repos:
        required: true
        default: 'swebench'
      run_id:
        required: true
        default: 'validate-gold'
      predictions_path:
        required: true
        default: 'gold'
      instance_ids:
        required: true
        default: 'sympy__sympy-20590'
      max_workers:
        required: true
        default: 1

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  run_swebench_evaluation:
    if: contains(github.event.inputs.choose_repos, 'swebench')
    env:
      REPO_NAME: princeton-nlp/SWE-bench
      REPO_PATH: SWE-bench-repo
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04]
        python-version: ["3.12"]
    runs-on: ${{ matrix.os }}
    defaults:
      run:
        working-directory: ${{ env.REPO_PATH }}
    timeout-minutes: 10
    steps:
    - name: Checkout repository ${{ env.REPO_NAME }}
      uses: actions/checkout@v4
      with:
        repository: ${{ env.REPO_NAME }}
        path: ${{ env.REPO_PATH }}
    - name: Setup Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    - name: Install SWE-bench
      run: |
        pip install --upgrade pip wheel
        pip install -e .
    - name: Run tests
      run: |
        python -m swebench.harness.run_evaluation \
          --predictions_path ${{ github.event.inputs.predictions_path }} \
          --max_workers ${{ github.event.inputs.max_workers }} \
          --instance_ids ${{ github.event.inputs.instance_ids }} \
          --run_id ${{ github.event.inputs.run_id }}
    - name: View tests
      run: |
        ls -R ./run_instance_logs/${{ github.event.inputs.run_id }}/${{ github.event.inputs.predictions_path }}
        for file in $(find ./run_instance_logs -type f); do
          echo "Contents of $file:"
          cat "$file"
          echo ""
        done
